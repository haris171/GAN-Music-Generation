{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from music21 import converter, instrument, note, chord\n",
    "import glob\n",
    "# import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(Generator, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "class MIDIDataset(Dataset):\n",
    "    def __init__(self, folder_path):\n",
    "        self.folder_path = folder_path\n",
    "        self.notes, self.durations, self.offsets = self.get_notes()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.notes)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.notes[idx], self.durations[idx], self.offsets[idx]\n",
    "\n",
    "    def get_notes(self):\n",
    "        \"\"\" Get all the notes and chords from the MIDI files in the specified directory \"\"\"\n",
    "        notes = []\n",
    "        durations = []\n",
    "        offsets = []\n",
    "\n",
    "        for file in glob.glob(self.folder_path + \"/*.mid\"):\n",
    "            try:\n",
    "                midi = converter.parse(file)\n",
    "\n",
    "                notes_to_parse = None\n",
    "\n",
    "                try:  # file has instrument parts\n",
    "                    s2 = instrument.partitionByInstrument(midi)\n",
    "                    notes_to_parse = s2.parts[0].recurse()\n",
    "                except:  # file has notes in a flat structure\n",
    "                    notes_to_parse = midi.flat.notes\n",
    "\n",
    "                for element in notes_to_parse:\n",
    "                    if isinstance(element, note.Note):\n",
    "                        notes.append(str(element.pitch))\n",
    "                        durations.append(element.duration.quarterLength)\n",
    "                        offsets.append(element.offset)\n",
    "                    elif isinstance(element, chord.Chord):\n",
    "                        notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "                        durations.append(element.duration.quarterLength)\n",
    "                        offsets.append(element.offset)\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing MIDI file {file}: {str(e)}\")\n",
    "\n",
    "        return notes, durations, offsets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing MIDI file Music/mendel_op19_2.mid: 5145200480\n",
      "Error parsing MIDI file Music/chpn_op33_2.mid: 5052874816\n",
      "Error parsing MIDI file Music/chpn_op35_2.mid: 5070946368\n",
      "Error parsing MIDI file Music/brahms_opus1_3.mid: 5638567104\n",
      "Error parsing MIDI file Music/schub_d760_3.mid: 5103047392\n"
     ]
    }
   ],
   "source": [
    "# Set device to GPU, if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 128\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "num_layers = 1\n",
    "num_epochs = 100\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Create DataLoader for the dataset\n",
    "data = MIDIDataset(\"Music\")\n",
    "dataloader = DataLoader(data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# Initialize models, optimizer, and loss function\n",
    "generator = Generator(input_size, hidden_size, output_size, num_layers)\n",
    "discriminator = Discriminator(input_size, hidden_size, output_size, num_layers)\n",
    "criterion = nn.BCELoss()\n",
    "gen_optimizer = optim.Adam(generator.parameters(), lr=learning_rate)\n",
    "disc_optimizer = optim.Adam(discriminator.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(epoch)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (notes, durations, offsets) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Convert data to tensors\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     notes_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnotes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     durations_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(durations, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m      9\u001b[0m     offsets_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(offsets, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n",
      "\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    print(epoch)\n",
    "    for i, (notes, durations, offsets) in enumerate(dataloader):\n",
    "\n",
    "        # Convert data to tensors\n",
    "        notes_tensor = torch.tensor(notes, dtype=torch.long)\n",
    "        durations_tensor = torch.tensor(durations, dtype=torch.long)\n",
    "        offsets_tensor = torch.tensor(offsets, dtype=torch.long)\n",
    "\n",
    "        \n",
    "\n",
    "        # Train Discriminator\n",
    "        disc_optimizer.zero_grad()\n",
    "        real_outputs = discriminator(notes_tensor)\n",
    "        real_labels = torch.ones(batch_size, 1)\n",
    "        real_loss = criterion(real_outputs, real_labels)\n",
    "        \n",
    "        fake_inputs = torch.randn(batch_size, input_size)  # Generate fake inputs\n",
    "        fake_notes = generator(fake_inputs)\n",
    "        fake_outputs = discriminator(fake_notes.detach())  # Detach generator gradients\n",
    "        fake_labels = torch.zeros(batch_size, 1)\n",
    "        fake_loss = criterion(fake_outputs, fake_labels)\n",
    "        \n",
    "        disc_loss = real_loss + fake_loss\n",
    "        disc_loss.backward()\n",
    "        disc_optimizer.step()\n",
    "\n",
    "        # Train Generator\n",
    "        gen_optimizer.zero_grad()\n",
    "        fake_outputs = discriminator(fake_notes)\n",
    "        gen_loss = criterion(fake_outputs, real_labels)\n",
    "        gen_loss.backward()\n",
    "        gen_optimizer.step()\n",
    "\n",
    "        if (i+1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(dataloader)}], \"\n",
    "                  f\"Gen Loss: {gen_loss.item():.4f}, Disc Loss: {disc_loss.item():.4f}\")\n",
    "\n",
    "# Save models\n",
    "torch.save(generator.state_dict(), 'generator.pth')\n",
    "torch.save(discriminator.state_dict(), 'discriminator.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
